from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.window import Window

# Load the full dataset
data_path = "/Volumes/workspace/default/signal_data/"
df = spark.read.option("header", True).option("inferSchema", True).csv(data_path)

# Sort by timestamp (optional, but good for windowing)
df = df.orderBy("timestamp")

# Add row index for batching
window_spec = Window.orderBy("timestamp")
df = df.withColumn("row_num", F.row_number().over(window_spec))

# Set batch size (e.g., 10000 rows per chunk)
batch_size = 10000
df = df.withColumn("batch_id", (F.col("row_num") / batch_size).cast("int"))

# Preview
df.select("timestamp", "batch_id").show(5)





import numpy as np
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras import regularizers
import pandas as pd

# Prepare variables
signal_cols = [c for c in df.columns if c.startswith("signal_")]
batch_ids = df.select("batch_id").distinct().orderBy("batch_id").rdd.flatMap(lambda x: x).collect()



from pyspark.ml.feature import VectorAssembler, MinMaxScaler
from pyspark.ml import Pipeline

# ✅ List of signal columns — update if different
signal_columns = [col for col in df.columns if col.startswith("sig_")]

# ✅ Step 1: Assemble all signals into a single vector column
assembler = VectorAssembler(inputCols=signal_columns, outputCol="features_vector")

# ✅ Step 2: Scale the vector using MinMaxScaler
scaler = MinMaxScaler(inputCol="features_vector", outputCol="scaled_features")

# ✅ Step 3: Build a pipeline
pipeline = Pipeline(stages=[assembler, scaler])

# ✅ Step 4: Fit and transform the data
pipeline_model = pipeline.fit(df)
scaled_df = pipeline_model.transform(df)

# ✅ Step 5: Select only needed columns
scaled_df = scaled_df.select("timestamp", "scaled_features")

# ✅ Show a few rows
display(scaled_df.limit(5))


# Model structure - define once
def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    encoder = Dense(32, activation="relu")(input_layer)
    encoder = Dense(16, activation="relu")(encoder)
    bottleneck = Dense(8, activation="relu")(encoder)
    decoder = Dense(16, activation="relu")(bottleneck)
    decoder = Dense(32, activation="relu")(decoder)
    output_layer = Dense(input_dim)(decoder)

    autoencoder = Model(inputs=input_layer, outputs=output_layer)
    autoencoder.compile(optimizer="adam", loss="mse")
    return autoencoder

autoencoder = build_autoencoder(len(signal_cols))
scaler = StandardScaler()

# Loop through each batch
for batch_id in batch_ids:
    print(f"Processing batch {batch_id}")
    
    # Step 1: Extract batch as Spark DataFrame
    batch_df = df.filter(F.col("batch_id") == batch_id).select(signal_cols)
    
    # Step 2: Convert to Pandas
    pdf = batch_df.toPandas()
    
    # Step 3: Drop rows with nulls (optional cleanup)
    pdf = pdf.dropna()
    
    if len(pdf) == 0:
        continue

    # Step 4: Normalize
    scaled = scaler.fit_transform(pdf)

    # Step 5: Train autoencoder on this batch
    autoencoder.fit(scaled, scaled, epochs=5, batch_size=256, verbose=1)

























from pyspark.ml.feature import VectorAssembler, MinMaxScaler
from pyspark.ml import Pipeline

# ✅ List of signal columns — update if different
signal_columns = [col for col in df.columns if col.startswith("sig_")]

# ✅ Step 1: Assemble all signals into a single vector column
assembler = VectorAssembler(inputCols=signal_columns, outputCol="features_vector")

# ✅ Step 2: Scale the vector using MinMaxScaler
scaler = MinMaxScaler(inputCol="features_vector", outputCol="scaled_features")

# ✅ Step 3: Build a pipeline
pipeline = Pipeline(stages=[assembler, scaler])

# ✅ Step 4: Fit and transform the data
pipeline_model = pipeline.fit(df)
scaled_df = pipeline_model.transform(df)

# ✅ Step 5: Select only needed columns
scaled_df = scaled_df.select("timestamp", "scaled_features")

# ✅ Show a few rows
display(scaled_df.limit(5))




# Databricks notebook
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler, MinMaxScaler
import mlflow
import mlflow.keras
from tensorflow import keras
import numpy as np

# -------------------------
# 1. Spark session (Databricks auto-provides this, no need to start)
# -------------------------
spark = SparkSession.builder.getOrCreate()

# -------------------------
# 2. Load CSV dataset from DBFS
# -------------------------
data_path = "dbfs:/FileStore/data/signal_dataset.csv"

df = spark.read.csv(data_path, header=True, inferSchema=True)

# Convert all signal columns to double
signal_cols = [c for c in df.columns if c != "Timestamp"]
df = df.select(col("Timestamp"), *[col(c).cast("double") for c in signal_cols])

# -------------------------
# 3. Assemble + scale
# -------------------------
assembler = VectorAssembler(inputCols=signal_cols, outputCol="features_raw")
df_assembled = assembler.transform(df)

scaler = MinMaxScaler(inputCol="features_raw", outputCol="features")
scaler_model = scaler.fit(df_assembled)
df_scaled = scaler_model.transform(df_assembled)

# Collect features as numpy for training
X = np.array(df_scaled.select("features").rdd.map(lambda x: x[0].toArray()).collect())

# -------------------------
# 4. Build Autoencoder
# -------------------------
input_dim = X.shape[1]
autoencoder = keras.Sequential([
    keras.layers.Input(shape=(input_dim,)),
    keras.layers.Dense(40, activation='relu'),
    keras.layers.Dense(20, activation='relu'),
    keras.layers.Dense(10, activation='relu'),
    keras.layers.Dense(20, activation='relu'),
    keras.layers.Dense(40, activation='relu'),
    keras.layers.Dense(input_dim, activation='sigmoid')
])

autoencoder.compile(optimizer='adam', loss='mse')

# -------------------------
# 5. Train & Log to MLflow
# -------------------------
mlflow.set_experiment("/Shared/Autoencoder_Exp")  # experiment path in Databricks

with mlflow.start_run():
    # Log parameters
    mlflow.log_param("input_dim", input_dim)
    mlflow.log_param("optimizer", "adam")
    mlflow.log_param("loss", "mse")
    mlflow.log_param("epochs", 20)
    mlflow.log_param("batch_size", 64)
    
    # Train
    history = autoencoder.fit(X, X, 
                               epochs=20, 
                               batch_size=64, 
                               shuffle=True, 
                               validation_split=0.1,
                               verbose=1)
    
    # Log metrics
    mlflow.log_metric("final_loss", history.history['loss'][-1])
    mlflow.log_metric("final_val_loss", history.history['val_loss'][-1])
    
    # Log the Keras model
    mlflow.keras.log_model(autoencoder, "model")
    
    # OPTIONAL: Save preprocessing objects (assembler, scaler) to MLflow artifacts
    import joblib
    import tempfile, os
    with tempfile.TemporaryDirectory() as tmp_dir:
        joblib.dump(assembler, os.path.join(tmp_dir, "assembler.pkl"))
        joblib.dump(scaler_model, os.path.join(tmp_dir, "scaler.pkl"))
        mlflow.log_artifacts(tmp_dir, artifact_path="preprocessing")

print("✅ Model and preprocessing pipeline saved to MLflow")
