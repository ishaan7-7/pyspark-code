from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

spark = SparkSession.builder.appName("KafkaStreamTest").getOrCreate()

# ðŸ”¹ Replace with your ngrok host:port
BOOTSTRAP = "0.tcp.ngrok.io:17890"
TOPIC = "iot_signals"

# Read from Kafka
df_raw = (spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", BOOTSTRAP)
    .option("subscribe", TOPIC)
    .option("startingOffsets", "latest")  # only new messages
    .load())

# Define schema: timestamp + 60 signals
schema = StructType([StructField("timestamp", StringType(), True)] +
                    [StructField(f"signal_{i}", DoubleType(), True) for i in range(1, 61)])

# Parse JSON messages
df = (df_raw
      .selectExpr("CAST(value AS STRING) AS json")
      .select(from_json(col("json"), schema).alias("data"))
      .select("data.*"))

# Show only timestamp + first 3 signals for sanity check
q = (df.select("timestamp", "signal_1", "signal_2", "signal_3")
     .writeStream
     .outputMode("append")
     .format("console")
     .option("truncate", False)
     .start())

q.awaitTermination()




JUPYTER CONSUMER

from confluent_kafka import Consumer
import json

# ðŸ”¹ Use your ngrok host:port here
BOOTSTRAP_SERVERS = "0.tcp.in.ngrok.io:13296"
TOPIC = "iot_signals"

conf = {
    'bootstrap.servers': BOOTSTRAP_SERVERS,
    'group.id': 'test-consumer',
    'auto.offset.reset': 'latest',   # only new messages
}

consumer = Consumer(conf)
consumer.subscribe([TOPIC])

print(f"Connected to {BOOTSTRAP_SERVERS}, listening to {TOPIC}...")

try:
    while True:
        msg = consumer.poll(1.0)  # wait up to 1s
        if msg is None:
            continue
        if msg.error():
            print("Error:", msg.error())
            continue

        # Decode JSON payload
        data = json.loads(msg.value().decode('utf-8'))
        print(data)   # print the whole message (timestamp + signals)

except KeyboardInterrupt:
    pass
finally:
    consumer.close()



JUPYTER CONSUMER

import datetime, random

# -----------------------
# Signal ranges
# -----------------------
signal_ranges = {}
for i in range(1, 11):
    signal_ranges[f"signal_{i}"] = (0, 1)
for i in range(11, 21):
    signal_ranges[f"signal_{i}"] = (0, 100)
for i in range(21, 41):
    signal_ranges[f"signal_{i}"] = (-50, 50)
for i in range(41, 61):
    signal_ranges[f"signal_{i}"] = (-100, 100)

# -----------------------
# Base timestamp in microseconds
# -----------------------
base_time = int(datetime.datetime.now().timestamp() * 1_000_000)

# -----------------------
# Stream generator function
# -----------------------
def generate_rows(batch_size=1000):
    """Generate a batch of signal rows with millisecond timestamp differences"""
    global base_time
    rows = []
    for _ in range(batch_size):
        step = random.randint(0, 5000)  # 0-5 ms
        base_time += step
        ts = datetime.datetime.fromtimestamp(base_time / 1_000_000).strftime("%m/%d/%Y %H:%M:%S:%f")
        row = [ts]
        for sig, (low, high) in signal_ranges.items():
            row.append(round(random.uniform(low, high), 4))
        rows.append(row)
    return rows

Batch to DF

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# -----------------------
# Spark session
# -----------------------
spark = SparkSession.builder.appName("SignalStreamSim").getOrCreate()

# -----------------------
# Schema for Spark DataFrame
# -----------------------
fields = [StructField("timestamp", StringType(), True)] + \
         [StructField(f"signal_{i}", DoubleType(), True) for i in range(1, 61)]
schema = StructType(fields)

# -----------------------
# Function to convert batch to Spark DF
# -----------------------
def batch_to_spark_df(rows):
    return spark.createDataFrame(rows, schema=schema)
