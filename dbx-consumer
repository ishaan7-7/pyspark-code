from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

spark = SparkSession.builder.appName("KafkaStreamTest").getOrCreate()

# ðŸ”¹ Replace with your ngrok host:port
BOOTSTRAP = "0.tcp.ngrok.io:17890"
TOPIC = "iot_signals"

# Read from Kafka
df_raw = (spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", BOOTSTRAP)
    .option("subscribe", TOPIC)
    .option("startingOffsets", "latest")  # only new messages
    .load())

# Define schema: timestamp + 60 signals
schema = StructType([StructField("timestamp", StringType(), True)] +
                    [StructField(f"signal_{i}", DoubleType(), True) for i in range(1, 61)])

# Parse JSON messages
df = (df_raw
      .selectExpr("CAST(value AS STRING) AS json")
      .select(from_json(col("json"), schema).alias("data"))
      .select("data.*"))

# Show only timestamp + first 3 signals for sanity check
q = (df.select("timestamp", "signal_1", "signal_2", "signal_3")
     .writeStream
     .outputMode("append")
     .format("console")
     .option("truncate", False)
     .start())

q.awaitTermination()
