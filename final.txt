from pyspark.sql.functions import col, to_timestamp, lit, explode, sequence, last
from pyspark.sql import Window
from pyspark.sql.types import TimestampType
from pyspark.sql import Row
from datetime import timedelta

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

raw_df = spark.read.option("header", "true").csv("/mnt/data/sensor_data.csv")

df = raw_df.withColumn("tagvalue", col("tagvalue").cast("double")) \
         .withColumn("start", col("start").cast("double")) \
         .withColumn("end", col("end").cast("double")) \
         .withColumn("tagtimestamp", to_timestamp(col("tagtimestamp"), "MM/dd/yyyy HH:mm:ss:SSS"))

min_ts = df.agg({"tagtimestamp": "min"}).first()[0]
max_ts = df.agg({"tagtimestamp": "max"}).first()[0]

time_diff_sec = int((max_ts.timestamp() - min_ts.timestamp())) + 1

sec_range = [Row(ff_tagtimestamp=min_ts + timedelta(seconds=i)) for i in range(time_diff_sec)]
sec_range_df = spark.createDataFrame(sec_range)

sensor_df = spark.createDataFrame([(s,) for s in sensor_list], ["tagid"])

full_timeline_df = sec_range_df.crossJoin(sensor_df)

seed_rows = [Row(ff_tagtimestamp=min_ts, tagid=s, tagvalue=0.0, start=0.0, end=0.0) for s in sensor_list]
seed_df = spark.createDataFrame(seed_rows)

df_prepared = df.withColumn("ff_tagtimestamp", col("tagtimestamp")) \
                .select("ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

df_prepared = df_prepared.unionByName(seed_df)

merged_df = full_timeline_df.join(df_prepared, on=["ff_tagtimestamp", "tagid"], how="left")

window = Window.partitionBy("tagid").orderBy("ff_tagtimestamp").rowsBetween(Window.unboundedPreceding, 0)

filled_df = merged_df.withColumn("tagvalue", last("tagvalue", ignorenulls=True).over(window)) \
                     .withColumn("start", last("start", ignorenulls=True).over(window)) \
                     .withColumn("end", last("end", ignorenulls=True).over(window)) \
                     .withColumn("tagtimestamp", last("ff_tagtimestamp", ignorenulls=True).over(window))

final_df = filled_df.select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end").orderBy("ff_tagtimestamp", "tagid")

display(final_df)
