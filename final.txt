from pyspark.sql.functions import col, to_timestamp, lit, explode, sequence, expr
from pyspark.sql import Window
from datetime import timedelta

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

raw_df = spark.read.option("header", "true").csv("/mnt/data/sensor_data.csv")

df = raw_df.withColumn("tagvalue", col("tagvalue").cast("double")) \
         .withColumn("start", col("start").cast("double")) \
         .withColumn("end", col("end").cast("double")) \
         .withColumn("tagtimestamp", to_timestamp(col("tagtimestamp"), "MM/dd/yyyy HH:mm:ss:SSS"))

df = df.orderBy("tagtimestamp")

df_rdd = df.rdd.zipWithIndex().map(lambda x: (x[1], x[0]))
df_dict = df_rdd.collectAsMap()

ff_rows = []
sensor_last_value = {sensor: 0.0 for sensor in sensor_list}

for i in range(len(df_dict) - 1):
    row1 = df_dict[i]
    row2 = df_dict[i + 1]

    t1 = row1["tagtimestamp"]
    t2 = row2["tagtimestamp"]

    existing_sensors = set()
    for j in range(i, i + 100):
        if j >= len(df_dict):
            break
        r = df_dict[j]
        if r["tagtimestamp"] > t2:
            break
        if t1 <= r["tagtimestamp"] <= t2:
            existing_sensors.add(r["tagid"])
            sensor_last_value[r["tagid"]] = r["tagvalue"]

    missing_sensors = set(sensor_list) - existing_sensors

    seconds_gap = int((t2 - t1).total_seconds())
    for sec in range(1, seconds_gap):
        fill_time = t1 + timedelta(seconds=sec)
        for sensor in missing_sensors:
            ff_rows.append((t1, fill_time, sensor, sensor_last_value[sensor], None, None))

original_df = df.withColumn("ff_tagtimestamp", col("tagtimestamp")) \
               .select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

schema = original_df.schema
ff_df = spark.createDataFrame(ff_rows, schema)

final_df = original_df.unionByName(ff_df).orderBy("ff_tagtimestamp", "tagid")
