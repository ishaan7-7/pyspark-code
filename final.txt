from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
from collections import defaultdict
from datetime import timedelta

spark = SparkSession.builder.getOrCreate()

df = spark.read.option("header", "true").csv("/path/to/input.csv", inferSchema=True)

df = df.withColumn(
    "tagtimestamp",
    unix_timestamp("tagtimestamp", "MM/dd/yyyy HH:mm:ss:SSS").cast(TimestampType())
)

expected_sensors = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

data = df.orderBy("tagtimestamp").collect()
last_seen_values = {sensor: 0.0 for sensor in expected_sensors}
final_rows = []

timestamp_map = defaultdict(list)
for row in data:
    timestamp_map[row["tagtimestamp"]].append(row)

sorted_timestamps = sorted(timestamp_map.keys())

for i in range(len(sorted_timestamps) - 1):
    ts_current = sorted_timestamps[i]
    ts_next = sorted_timestamps[i + 1]
    gap = int((ts_next - ts_current).total_seconds())

    for row in timestamp_map[ts_current]:
        tagid = row["tagid"]
        tagvalue = row["tagvalue"]
        last_seen_values[tagid] = tagvalue
        final_rows.append((ts_current, ts_current, tagid, tagvalue, row["start"], row["end"]))

    present = {r["tagid"] for r in timestamp_map[ts_current]}
    missing_sensors = set(expected_sensors) - present

    for sec in range(1, gap):
        ff_time = ts_current + timedelta(seconds=sec)
        for sensor in missing_sensors:
            final_rows.append((ts_current, ff_time, sensor, last_seen_values[sensor], None, None))

last_ts = sorted_timestamps[-1]
for row in timestamp_map[last_ts]:
    tagid = row["tagid"]
    tagvalue = row["tagvalue"]
    last_seen_values[tagid] = tagvalue
    final_rows.append((last_ts, last_ts, tagid, tagvalue, row["start"], row["end"]))

schema = StructType([
    StructField("tagtimestamp", TimestampType(), True),
    StructField("ff_tagtimestamp", TimestampType(), True),
    StructField("tagid", StringType(), True),
    StructField("tagvalue", DoubleType(), True),
    StructField("start", DoubleType(), True),
    StructField("end", DoubleType(), True)
])

filled_df = spark.createDataFrame(final_rows, schema=schema)
filled_df = filled_df.orderBy("ff_tagtimestamp", "tagid")
filled_df.show(100, truncate=False)
