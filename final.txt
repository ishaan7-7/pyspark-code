from pyspark.sql.functions import col, to_timestamp, lit, explode, sequence, expr, struct
from pyspark.sql import Window
from datetime import timedelta

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

raw_df = spark.read.option("header", "true").csv("/mnt/data/sensor_data.csv")

df = raw_df.withColumn("tagvalue", col("tagvalue").cast("double")) \
         .withColumn("start", col("start").cast("double")) \
         .withColumn("end", col("end").cast("double")) \
         .withColumn("tagtimestamp", to_timestamp(col("tagtimestamp"), "MM/dd/yyyy HH:mm:ss:SSS"))

w = Window.orderBy("tagtimestamp")
df = df.withColumn("next_timestamp", expr("lead(tagtimestamp) over (order by tagtimestamp)"))

df = df.filter(col("next_timestamp").isNotNull())

df_exploded = df.select(
    col("tagtimestamp").alias("window_start"),
    col("next_timestamp").alias("window_end"),
    explode(sequence(expr("window_start + interval 1 seconds"), expr("window_end - interval 1 seconds"))).alias("ff_tagtimestamp")
)

df_exploded = df_exploded.crossJoin(
    spark.createDataFrame([(s,) for s in sensor_list], ["tagid"])
)

existing = df.select(col("tagtimestamp").alias("ff_tagtimestamp"), "tagid")
df_exploded = df_exploded.join(existing, ["ff_tagtimestamp", "tagid"], "left_anti")

last_value_window = Window.partitionBy("tagid").orderBy("tagtimestamp").rowsBetween(Window.unboundedPreceding, 0)

df_with_last = df.withColumn("last_tagvalue", expr("last(tagvalue, true) over (partition by tagid order by tagtimestamp rows between unbounded preceding and current row)"))
latest_vals = df_with_last.groupBy("tagid").agg(expr("last(tagvalue, true)").alias("tagvalue"))

fill_vals = df_exploded.join(latest_vals, ["tagid"], "left") \
    .withColumn("tagvalue", col("tagvalue").cast("double")) \
    .withColumn("tagtimestamp", col("window_start")) \
    .withColumn("start", lit(None).cast("double")) \
    .withColumn("end", lit(None).cast("double")) \
    .select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

existing_ff = df.withColumn("ff_tagtimestamp", col("tagtimestamp")) \
               .select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

final_df = existing_ff.unionByName(fill_vals).orderBy("ff_tagtimestamp", "tagid")
