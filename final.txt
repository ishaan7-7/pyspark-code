from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, explode, array, struct, last, asc
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("Sensor Forward Fill").getOrCreate()

df = spark.read.option("header", True).option("inferSchema", True).csv("/path/to/your/data.csv")

unique_times_df = df.select("tagtime").distinct()

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

sensors_df = spark.createDataFrame([(s,) for s in sensor_list], ["tagid"])

grid_df = unique_times_df.crossJoin(sensors_df)

enriched_df = grid_df.join(df.select("tagtime", "tagid", "tagvalue", "start", "end"), 
                           on=["tagtime", "tagid"], how="left")

window_spec = Window.partitionBy("tagid").orderBy(asc("tagtime")).rowsBetween(Window.unboundedPreceding, 0)

filled_df = enriched_df.withColumn("tagvalue_filled", last("tagvalue", ignorenulls=True).over(window_spec))

filled_df = filled_df.withColumn("tagvalue_final", col("tagvalue_filled").cast("double")).fillna({"tagvalue_final": 0.0})

pivot_df = filled_df.select("tagtime", "tagid", "tagvalue_final") \
                    .groupBy("tagtime") \
                    .pivot("tagid") \
                    .agg({"tagvalue_final": "first"}) \
                    .orderBy("tagtime")

pivot_df.show(truncate=False)
