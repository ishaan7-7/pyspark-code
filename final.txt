from pyspark.sql.functions import col, to_timestamp, lit, explode, expr, sequence, last
from pyspark.sql import Window
from pyspark.sql.types import TimestampType
from pyspark.sql import Row
from datetime import timedelta

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

raw_df = spark.read.option("header", "true").csv("/mnt/data/sensor_data.csv")

df = raw_df.withColumn("tagvalue", col("tagvalue").cast("double")) \
         .withColumn("start", col("start").cast("double")) \
         .withColumn("end", col("end").cast("double")) \
         .withColumn("tagtimestamp", to_timestamp(col("tagtimestamp"), "MM/dd/yyyy HH:mm:ss:SSS"))

min_ts = df.selectExpr("min(tagtimestamp) as min_ts").first()["min_ts"]
max_ts = df.selectExpr("max(tagtimestamp) as max_ts").first()["max_ts"]

sec_range_df = spark.range(0, int((max_ts - min_ts).total_seconds()) + 1).withColumn("ff_tagtimestamp", expr(f"timestamp'{min_ts}' + interval cast(id as int) seconds"))

sensor_df = spark.createDataFrame([(s,) for s in sensor_list], ["tagid"])

full_timeline_df = sec_range_df.crossJoin(sensor_df)

seed_rows = [Row(ff_tagtimestamp=min_ts, tagid=s, tagvalue=0.0, start=0.0, end=0.0) for s in sensor_list]
seed_df = spark.createDataFrame(seed_rows)

df_prepared = df.withColumn("ff_tagtimestamp", col("tagtimestamp")) \
                .select("ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

df_prepared = df_prepared.unionByName(seed_df)

merged_df = full_timeline_df.join(df_prepared, on=["ff_tagtimestamp", "tagid"], how="left")

window = Window.partitionBy("tagid").orderBy("ff_tagtimestamp").rowsBetween(Window.unboundedPreceding, 0)

filled_df = merged_df.withColumn("tagvalue", last("tagvalue", ignorenulls=True).over(window)) \
                     .withColumn("start", last("start", ignorenulls=True).over(window)) \
                     .withColumn("end", last("end", ignorenulls=True).over(window)) \
                     .withColumn("tagtimestamp", expr("first(ff_tagtimestamp) over (partition by tagid order by ff_tagtimestamp rows between current row and current row)"))

final_df = filled_df.select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end").orderBy("ff_tagtimestamp", "tagid")
