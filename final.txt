from pyspark.sql.functions import col, to_timestamp, lit, explode, expr, lead, collect_set, udf, array, struct
from pyspark.sql import Window
from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DoubleType, TimestampType
from datetime import timedelta, datetime

sensor_list = [
    "VDLM1VS1.XRMSvelocity", "VDLM1VS1.ZRMSvelocity", "VDLM1VS2.XRMSvelocity", "VDLM1VS2.ZRMSvelocity",
    "VDLM2VS1.XRMSvelocity", "VDLM2VS1.ZRMSvelocity", "VDLM2VS2.XRMSvelocity", "VDLM2VS2.ZRMSvelocity",
    "OutputcurrentVFD1", "OutputcurrentIDC1", "BusvoltageIDC1", "TorqueFeedbackVFD1",
    "OutputcurrentVFD2", "OutputcurrentIDC2", "BusvoltageIDC2", "TorqueFeedbackVFD2"
]

raw_df = spark.read.option("header", "true").csv("/mnt/data/sensor_data.csv")

df = raw_df.withColumn("tagvalue", col("tagvalue").cast("double")) \
         .withColumn("start", col("start").cast("double")) \
         .withColumn("end", col("end").cast("double")) \
         .withColumn("tagtimestamp", to_timestamp(col("tagtimestamp"), "MM/dd/yyyy HH:mm:ss:SSS"))

window = Window.orderBy("tagtimestamp")
df = df.withColumn("next_tagtimestamp", lead("tagtimestamp").over(window))

sensor_last_value = {s: 0.0 for s in sensor_list}

def generate_missing_rows(start_ts, end_ts, sensors_missing):
    result = []
    if start_ts is None or end_ts is None:
        return result
    step = timedelta(seconds=1)
    current = start_ts + step
    while current < end_ts:
        for s in sensors_missing:
            val = sensor_last_value.get(s, 0.0)
            result.append((start_ts, current, s, val, None, None))
        current += step
    return result

generate_missing_rows_udf = udf(generate_missing_rows, ArrayType(StructType([
    StructField("tagtimestamp", TimestampType()),
    StructField("ff_tagtimestamp", TimestampType()),
    StructField("tagid", StringType()),
    StructField("tagvalue", DoubleType()),
    StructField("start", DoubleType()),
    StructField("end", DoubleType()),
])))

sensor_set_df = df.groupBy("tagtimestamp").agg(collect_set("tagid").alias("present_sensors"))

pair_df = df.select("tagtimestamp", "next_tagtimestamp").distinct().na.drop()
pair_df = pair_df.join(sensor_set_df, on="tagtimestamp", how="left")

@udf(ArrayType(StringType()))
def missing_sensors_udf(present):
    if present is None:
        return sensor_list
    return list(set(sensor_list) - set(present))

pair_df = pair_df.withColumn("missing_sensors", missing_sensors_udf("present_sensors"))

pair_df = pair_df.withColumn("filled_rows", generate_missing_rows_udf("tagtimestamp", "next_tagtimestamp", "missing_sensors"))

filled_df = pair_df.selectExpr("explode(filled_rows) as row").select(
    col("row.tagtimestamp"),
    col("row.ff_tagtimestamp"),
    col("row.tagid"),
    col("row.tagvalue"),
    col("row.start"),
    col("row.end")
)

original_df = df.withColumn("ff_tagtimestamp", col("tagtimestamp")) \
               .select("tagtimestamp", "ff_tagtimestamp", "tagid", "tagvalue", "start", "end")

final_df = original_df.unionByName(filled_df).orderBy("ff_tagtimestamp", "tagid")
